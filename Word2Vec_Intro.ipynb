{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word2Vec_Intro.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO4IWp2IhWMUwpUZ1fP5JrL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tawfiqam/MI564/blob/main/Word2Vec_Intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAHumz1Mb-GM",
        "outputId": "8a9927ce-06d9-4eba-a1e8-b446c0f44e81"
      },
      "source": [
        "!pip install psaw"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting psaw\n",
            "  Downloading https://files.pythonhosted.org/packages/01/fe/e2f43241ff7545588d07bb93dd353e4333ebc02c31d7e0dc36a8a9d93214/psaw-0.1.0-py3-none-any.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from psaw) (2.23.0)\n",
            "Requirement already satisfied: Click in /usr/local/lib/python3.7/dist-packages (from psaw) (7.1.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->psaw) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->psaw) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->psaw) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->psaw) (2020.12.5)\n",
            "Installing collected packages: psaw\n",
            "Successfully installed psaw-0.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "2VhF2L9abq5y",
        "outputId": "4336019a-2bca-4736-acce-80fbea8e71da"
      },
      "source": [
        "import pandas as pd\n",
        "#we will need datetime in order to specify the timeline we need to collect the data\n",
        "import datetime as dt\n",
        "#regular expressions will help us clean the text\n",
        "import re\n",
        "#gensim\n",
        "import gensim\n",
        "#LDA model\n",
        "from gensim.models import LdaModel\n",
        "#Use this to setup a dictionay\n",
        "from gensim.corpora import Dictionary\n",
        "#This will allow us to get the coherence models\n",
        "from gensim.models import CoherenceModel\n",
        "#gensim is a python library used mostly for NLP analysis\n",
        "#corpora allows us to build a corpus out of different \n",
        "#documents\n",
        "from gensim import corpora\n",
        "#Defaultdict is a container like dictionaries present in the module collections. \n",
        "#Defaultdict is a sub-class of the dict class that returns a dictionary-like object. \n",
        "#The functionality of both dictionaries and defualtdict are almost same except for \n",
        "#the fact that defualtdict never raises a KeyError. It provides a default value \n",
        "#for the key that does not exists.\n",
        "from collections import defaultdict\n",
        "\n",
        "#importing ntlk\n",
        "#we will be using nltk to create bag of words\n",
        "import nltk\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "#this recognizes where a sentence starts & ends\n",
        "#http://www.nltk.org/_modules/nltk/tokenize/punkt.html\n",
        "nltk.download('punkt')\n",
        "#Wordnet is important for Lemmatization ==> seel later when we dig into this\n",
        "#https://www.nltk.org/howto/wordnet.html\n",
        "nltk.download('wordnet')\n",
        "#use nltk to get stopwords\n",
        "nltk.download('stopwords')\n",
        "#create a stoplist that includes all the english stop words\n",
        "stopwords_english = stopwords.words('english')\n",
        "display('here is a subset of the stop words')\n",
        "display(stopwords_english[0:10])\n",
        "#now we import the wrapper in order to use the API\n",
        "# Text Cleaning\n",
        "import string\n",
        "\n",
        "from psaw import PushshiftAPI\n",
        "\n",
        "api = PushshiftAPI()\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'here is a subset of the stop words'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEjYMHkGFTnl"
      },
      "source": [
        "Lemmatization is the process of grouping together the inflected forms of a word so they can be analysed as a single item. For example, if we are looking through a document and want to do topic modeling, it is important that we do not see each inflection of the word differently. That would confuse the topic modeling algorithm. So, for example, we'd want to see the words \"studies\" \"studied\" and \"studying\" as \"study.\" [1,2]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hl5Hp-05FOBG"
      },
      "source": [
        "![lemma_v2.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjosT5kaHRKQ"
      },
      "source": [
        "Another simpler way of doing this is called stemming. Stemming algorithms work by cutting off the end or the beginning of the word, taking into account a list of common prefixes and suffixes that can be found in an inflected word. This indiscriminate cutting can be successful in some occasions, but not always, and that is why we affirm that this approach presents some limitations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0XX5MEXHkVx"
      },
      "source": [
        "![stemming_v2.png](data:image/png;base64,UklGRvgCAABXRUJQVlA4TOsCAAAv90AgACcgECD8/xkxYENIQPj/T/eEQIDw4D9Lj/kPIGOmKQFiA7hVnZPFAmEC427QLvBj/6GIHewjj/6liP5DkCQ3bjNMIQGKZAHUEaz9g/2XYwCHhxlYiNkveA0JvHavDEDhlXjKO4YQyunHQoSSIBE3lwZwxPsZ4XVBCbht8do9t12NBXA3CI5S8JpkcGVed5dcupAFgRh2F7/A6IR2d45IT4nJuVIKhwhQWfAaUkTYKX0JWex4HeOvY+lnMhacUL6H4Y8Fs7CDaYY1TAp2XE94AxUJufV5LLAPnoga4dkhGMDdPQPcTsVli3uvRH7nxpPRxpNRxzMpuR8+REWnBIZ7AkNmckSH2P0AmABHft2Dc9a29XgGgfY4A5cdoHBzoVlH1NRgHhg6zdk1A5wVCjDxhbiapIBFwrx1c9g54VTMSNgAmKVBmi2PklAKBiRorWXCAhUAzDAFg2bMMdaWMn4zzjIBB8lieYyEUZBRasNlq23nhDpMSYzO0S0rhXtimPC94k5R43vlsfuGHz5eHno89pQfPkQdLYnodeDR+droIfRkm7Xeo/ZO+cQRdX0QoHQ91OtHok9U19N5YBiwOXmwbmcg9J50PVTATvtjg9dD7ZRxZpfoIdGMHpoVpWAAC4FSEWgGgh4yiAb6FMsYA/QCPSQKyyAF3Kst53D0UAaGHup33tBsSYML9JBBCXpoQJyXNKJsPZQlMYb1DBknSCE1YLcHLDIMjxN0YT6if+30UPgvOyHonjJQzImIKsNR0SkdMdxWWBQ6EDrNf4BbAMTPcSG+/xPUgCEKnfAlyZpHlPk9jevg3U4l4/+OPVCzVnJCt8vBpeh24rKV0VlpreXDQdGih+fEhW1txbAJZsbK2LQcM7YaAtNZOXZo1feKXhL+2vcN/+X0kOuVWmt5Yo88pR6KiqQCfI/K1kPk3nwr9+Z0XXCLuBXNkghUAFfEtMb9rhg37xOia/6i9oZ7SU0K773yi6qHXgAA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Popsz0zZHyve"
      },
      "source": [
        "References:\n",
        "\n",
        "[1] https://en.wikipedia.org/wiki/Lemmatisation\n",
        "\n",
        "[2] [https://blog.bitext.com what-is-the-difference-between-stemming-and-lemmatization/]()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nf1zLL_iE-W3"
      },
      "source": [
        "#We are using nltk lemmatizer \n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sirxp9lGplKF"
      },
      "source": [
        "##Collecting reddit comments from Ex_Foster"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qOb0kJBcEDx"
      },
      "source": [
        "#this function will allow us to find the last day of each month\n",
        "#for example, there are 31 days in January, but 28 this February\n",
        "def last_day_of_month(any_day):\n",
        "    # this will never fail\n",
        "    # get close to the end of the month for any day, and add 4 days 'over'\n",
        "    next_month = any_day.replace(day=28) + datetime.timedelta(days=4)\n",
        "    # subtract the number of remaining 'overage' days to get last day of current month, or said programattically said, the previous day of the first of next month\n",
        "    return next_month - datetime.timedelta(days=next_month.day)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6z7WZUp4b7zQ",
        "outputId": "9a80c9a4-7eb6-444c-87a1-504044da80b6"
      },
      "source": [
        "import datetime\n",
        "subredditlist = ['Ex_Foster']\n",
        "for reddit in subredditlist:\n",
        "   #collect data between 2019 and 2020\n",
        "    for y in range(2019,2021):\n",
        "      #collect data between January and December\n",
        "      for i in range(1,13):\n",
        "          file_name= str(reddit)+\"_\"+str(y)+\"_\"+str(i)+\".json\"\n",
        "          print(\"starting with the month \"+str(i))\n",
        "          print(\"for subreddit...\"+str(reddit))\n",
        "          print(\"setting start epoch...\")\n",
        "          start_epoch=int(dt.datetime(y, i, 1).timestamp())\n",
        "          print(\"setting end epoch...\")\n",
        "          last_day = last_day_of_month(datetime.date(y, i, 1))\n",
        "          print(\"the last day of the month is...\")\n",
        "          print(last_day.day)\n",
        "          last_day = int(last_day.day)\n",
        "          end_epoch = int(dt.datetime(y,i,last_day).timestamp())\n",
        "          print(\"setting up the generator...\")\n",
        "          gen = api.search_comments(after=start_epoch, before=end_epoch,subreddit=reddit)\n",
        "          print(\"setting up the dataframe...\")\n",
        "          df = pd.DataFrame([obj.d_ for obj in gen])\n",
        "          print(\"The number of comments for year \"+ str(y)+\" and month \"+str(i)+\" is \"+str(len(df.index)))\n",
        "          df.to_json(file_name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "starting with the month 1\n",
            "for subreddit...Ex_Foster\n",
            "setting start epoch...\n",
            "setting end epoch...\n",
            "the last day of the month is...\n",
            "31\n",
            "setting up the generator...\n",
            "setting up the dataframe...\n",
            "The number of comments for year 2019 and month 1 is 0\n",
            "starting with the month 2\n",
            "for subreddit...Ex_Foster\n",
            "setting start epoch...\n",
            "setting end epoch...\n",
            "the last day of the month is...\n",
            "28\n",
            "setting up the generator...\n",
            "setting up the dataframe...\n",
            "The number of comments for year 2019 and month 2 is 0\n",
            "starting with the month 3\n",
            "for subreddit...Ex_Foster\n",
            "setting start epoch...\n",
            "setting end epoch...\n",
            "the last day of the month is...\n",
            "31\n",
            "setting up the generator...\n",
            "setting up the dataframe...\n",
            "The number of comments for year 2019 and month 3 is 69\n",
            "starting with the month 4\n",
            "for subreddit...Ex_Foster\n",
            "setting start epoch...\n",
            "setting end epoch...\n",
            "the last day of the month is...\n",
            "30\n",
            "setting up the generator...\n",
            "setting up the dataframe...\n",
            "The number of comments for year 2019 and month 4 is 233\n",
            "starting with the month 5\n",
            "for subreddit...Ex_Foster\n",
            "setting start epoch...\n",
            "setting end epoch...\n",
            "the last day of the month is...\n",
            "31\n",
            "setting up the generator...\n",
            "setting up the dataframe...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/psaw/PushshiftAPI.py:192: UserWarning: Got non 200 code 429\n",
            "  warnings.warn(\"Got non 200 code %s\" % response.status_code)\n",
            "/usr/local/lib/python3.7/dist-packages/psaw/PushshiftAPI.py:180: UserWarning: Unable to connect to pushshift.io. Retrying after backoff.\n",
            "  warnings.warn(\"Unable to connect to pushshift.io. Retrying after backoff.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The number of comments for year 2019 and month 5 is 119\n",
            "starting with the month 6\n",
            "for subreddit...Ex_Foster\n",
            "setting start epoch...\n",
            "setting end epoch...\n",
            "the last day of the month is...\n",
            "30\n",
            "setting up the generator...\n",
            "setting up the dataframe...\n",
            "The number of comments for year 2019 and month 6 is 183\n",
            "starting with the month 7\n",
            "for subreddit...Ex_Foster\n",
            "setting start epoch...\n",
            "setting end epoch...\n",
            "the last day of the month is...\n",
            "31\n",
            "setting up the generator...\n",
            "setting up the dataframe...\n",
            "The number of comments for year 2019 and month 7 is 78\n",
            "starting with the month 8\n",
            "for subreddit...Ex_Foster\n",
            "setting start epoch...\n",
            "setting end epoch...\n",
            "the last day of the month is...\n",
            "31\n",
            "setting up the generator...\n",
            "setting up the dataframe...\n",
            "The number of comments for year 2019 and month 8 is 80\n",
            "starting with the month 9\n",
            "for subreddit...Ex_Foster\n",
            "setting start epoch...\n",
            "setting end epoch...\n",
            "the last day of the month is...\n",
            "30\n",
            "setting up the generator...\n",
            "setting up the dataframe...\n",
            "The number of comments for year 2019 and month 9 is 229\n",
            "starting with the month 10\n",
            "for subreddit...Ex_Foster\n",
            "setting start epoch...\n",
            "setting end epoch...\n",
            "the last day of the month is...\n",
            "31\n",
            "setting up the generator...\n",
            "setting up the dataframe...\n",
            "The number of comments for year 2019 and month 10 is 351\n",
            "starting with the month 11\n",
            "for subreddit...Ex_Foster\n",
            "setting start epoch...\n",
            "setting end epoch...\n",
            "the last day of the month is...\n",
            "30\n",
            "setting up the generator...\n",
            "setting up the dataframe...\n",
            "The number of comments for year 2019 and month 11 is 388\n",
            "starting with the month 12\n",
            "for subreddit...Ex_Foster\n",
            "setting start epoch...\n",
            "setting end epoch...\n",
            "the last day of the month is...\n",
            "31\n",
            "setting up the generator...\n",
            "setting up the dataframe...\n",
            "The number of comments for year 2019 and month 12 is 326\n",
            "starting with the month 1\n",
            "for subreddit...Ex_Foster\n",
            "setting start epoch...\n",
            "setting end epoch...\n",
            "the last day of the month is...\n",
            "31\n",
            "setting up the generator...\n",
            "setting up the dataframe...\n",
            "The number of comments for year 2020 and month 1 is 134\n",
            "starting with the month 2\n",
            "for subreddit...Ex_Foster\n",
            "setting start epoch...\n",
            "setting end epoch...\n",
            "the last day of the month is...\n",
            "29\n",
            "setting up the generator...\n",
            "setting up the dataframe...\n",
            "The number of comments for year 2020 and month 2 is 210\n",
            "starting with the month 3\n",
            "for subreddit...Ex_Foster\n",
            "setting start epoch...\n",
            "setting end epoch...\n",
            "the last day of the month is...\n",
            "31\n",
            "setting up the generator...\n",
            "setting up the dataframe...\n",
            "The number of comments for year 2020 and month 3 is 275\n",
            "starting with the month 4\n",
            "for subreddit...Ex_Foster\n",
            "setting start epoch...\n",
            "setting end epoch...\n",
            "the last day of the month is...\n",
            "30\n",
            "setting up the generator...\n",
            "setting up the dataframe...\n",
            "The number of comments for year 2020 and month 4 is 279\n",
            "starting with the month 5\n",
            "for subreddit...Ex_Foster\n",
            "setting start epoch...\n",
            "setting end epoch...\n",
            "the last day of the month is...\n",
            "31\n",
            "setting up the generator...\n",
            "setting up the dataframe...\n",
            "The number of comments for year 2020 and month 5 is 501\n",
            "starting with the month 6\n",
            "for subreddit...Ex_Foster\n",
            "setting start epoch...\n",
            "setting end epoch...\n",
            "the last day of the month is...\n",
            "30\n",
            "setting up the generator...\n",
            "setting up the dataframe...\n",
            "The number of comments for year 2020 and month 6 is 185\n",
            "starting with the month 7\n",
            "for subreddit...Ex_Foster\n",
            "setting start epoch...\n",
            "setting end epoch...\n",
            "the last day of the month is...\n",
            "31\n",
            "setting up the generator...\n",
            "setting up the dataframe...\n",
            "The number of comments for year 2020 and month 7 is 353\n",
            "starting with the month 8\n",
            "for subreddit...Ex_Foster\n",
            "setting start epoch...\n",
            "setting end epoch...\n",
            "the last day of the month is...\n",
            "31\n",
            "setting up the generator...\n",
            "setting up the dataframe...\n",
            "The number of comments for year 2020 and month 8 is 217\n",
            "starting with the month 9\n",
            "for subreddit...Ex_Foster\n",
            "setting start epoch...\n",
            "setting end epoch...\n",
            "the last day of the month is...\n",
            "30\n",
            "setting up the generator...\n",
            "setting up the dataframe...\n",
            "The number of comments for year 2020 and month 9 is 217\n",
            "starting with the month 10\n",
            "for subreddit...Ex_Foster\n",
            "setting start epoch...\n",
            "setting end epoch...\n",
            "the last day of the month is...\n",
            "31\n",
            "setting up the generator...\n",
            "setting up the dataframe...\n",
            "The number of comments for year 2020 and month 10 is 222\n",
            "starting with the month 11\n",
            "for subreddit...Ex_Foster\n",
            "setting start epoch...\n",
            "setting end epoch...\n",
            "the last day of the month is...\n",
            "30\n",
            "setting up the generator...\n",
            "setting up the dataframe...\n",
            "The number of comments for year 2020 and month 11 is 100\n",
            "starting with the month 12\n",
            "for subreddit...Ex_Foster\n",
            "setting start epoch...\n",
            "setting end epoch...\n",
            "the last day of the month is...\n",
            "31\n",
            "setting up the generator...\n",
            "setting up the dataframe...\n",
            "The number of comments for year 2020 and month 12 is 136\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXRv9veCsPmW"
      },
      "source": [
        "##Cleaning text and creating documents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNDHghWQD5W6"
      },
      "source": [
        "#creating smaller dataframe\n",
        "#we do not need all the columns\n",
        "df = df[['id','link_id','body','created','author']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TM09mx0afJBi"
      },
      "source": [
        "def first_pass(text): \n",
        "    text = str(text)\n",
        "    #remove newline character code\n",
        "    text = text.replace('\\\\n',' ')\n",
        "    #remove the ampersand character code\n",
        "    text = text.replace('&amp',' ')\n",
        "    #character code for a Zero-witdh space, which is a character that acts like a spacebar, except it's invisible\n",
        "    #https://www.reddit.com/r/OutOfTheLoop/comments/9abjhm/what_does_x200b_mean/\n",
        "    text = text.replace(';#x200B;',' ')\n",
        "    #A commonly used character in HTML is the non-breaking space: &nbsp;\n",
        "    text = text.replace('nbsp',' ')\n",
        "    \n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhfVLSHpdRaf"
      },
      "source": [
        "df['clean_text'] = df.body.apply(first_pass)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnRKwcmPM18w"
      },
      "source": [
        "def second_pass(text): \n",
        "    #make string lowercase \n",
        "    text = str(text)\n",
        "    text = text.lower()\n",
        "    \n",
        "    #remove links\n",
        "    text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
        "\n",
        "    #tokenize\n",
        "    #create bag of words\n",
        "    #1-grams \n",
        "    tokens = nltk.word_tokenize(text) \n",
        "    clean_text = []\n",
        "    \n",
        "    #remove stopwords, puncuation, then lemmatize\n",
        "    for word in tokens:\n",
        "        if (word not in stopwords_english and word not in string.punctuation): \n",
        "            token = wordnet_lemmatizer.lemmatize(word)\n",
        "            clean_text.append(token)\n",
        "            \n",
        "    #remove words of length 3 or smaller        \n",
        "    clean_text = [token for token in clean_text if len(token) > 3] \n",
        "            \n",
        "    return clean_text      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ey8Xr1lM21Q"
      },
      "source": [
        "df['clean_text'] = df.clean_text.apply(second_pass)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OaJEG08uM8Zl"
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "model = Word2Vec(df.clean_text, min_count=5,size= 50,workers=3, window =10, sg = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kNkO4oFOW3t",
        "outputId": "f425df80-90b3-4d5b-84ef-73da01dd1d7c"
      },
      "source": [
        "model.most_similar('parent')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('said', 0.99928879737854),\n",
              " ('stay', 0.9992765784263611),\n",
              " ('possible', 0.9992459416389465),\n",
              " ('every', 0.9991785287857056),\n",
              " ('week', 0.9991408586502075),\n",
              " ('abuse', 0.9991047978401184),\n",
              " ('reading', 0.9991033673286438),\n",
              " ('friend', 0.9990967512130737),\n",
              " ('shit', 0.9990841746330261),\n",
              " ('book', 0.9990795254707336)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YKuIQ8wOr0G",
        "outputId": "5dd1e4bd-1f4d-493a-f254-d0a994601486"
      },
      "source": [
        "model.most_similar('school')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('talk', 0.9992128014564514),\n",
              " ('room', 0.9992107152938843),\n",
              " ('alone', 0.9991599321365356),\n",
              " ('part', 0.9991534352302551),\n",
              " ('called', 0.9991406798362732),\n",
              " ('help', 0.9991308450698853),\n",
              " ('definitely', 0.9991297721862793),\n",
              " ('week', 0.9991133809089661),\n",
              " ('worked', 0.9991123676300049),\n",
              " ('live', 0.9990778565406799)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "ugsDFtaGR_uN",
        "outputId": "d4ce359f-6eaf-4930-8aa3-69dd655c4461"
      },
      "source": [
        "model.doesnt_match(['sister', 'brother', 'school'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py:895: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'school'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "qDvvdV6MSmAL",
        "outputId": "5519392d-5875-4aae-80ed-8af667cd34f6"
      },
      "source": [
        "model.most_similar(positive=[\"foster\", \"woman\"], negative=[\"mother\"], topn=3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-caf5a5c1b5c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"foster\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"woman\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"mother\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/utils.py\u001b[0m in \u001b[0;36mnew_func1\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1420\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1421\u001b[0m                 )\n\u001b[0;32m-> 1422\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_func1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m   1395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1396\u001b[0m         \"\"\"\n\u001b[0;32m-> 1397\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestrict_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1399\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mdeprecated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Method will be removed in 4.0.0, use self.wv.wmdistance() instead\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    450\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"word 'mother' not in vocabulary\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8G0eHC_S1_t"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuB0OYpBGxh_"
      },
      "source": [
        "#We will consider all the comments in each of the reddit threads a document\n",
        "#link_id is used to identify each thread\n",
        "df_p = df.reset_index().groupby(\"link_id\",as_index=False )['body'].apply(lambda x: ','.join(x))\n",
        "\n",
        "p_threads = df_p.values\n",
        "\n",
        "df_p_threads = pd.DataFrame(p_threads)\n",
        "df_p_threads.rename(columns={0: \"comment_thread\", 1: \"body\"},inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3qtOS9XTI-o"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-X-4SfhWdDK",
        "outputId": "7830ecc9-701f-47cf-a1f0-64dbd81b4f2c"
      },
      "source": [
        "df_p_threads['body']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     Thank you!,I am a ffy and a teacher - keep my ...\n",
              "1     I loved both respite people I stayed with it w...\n",
              "2     I was only in foster care from age 16-17 only ...\n",
              "3     I'm really happy to hear that things were ship...\n",
              "4     Oh no, I'm so sorry for your loss. I am going ...\n",
              "5     Honestly, I don't know. I'm not accustomed to ...\n",
              "6     You are right these are systemic barriers that...\n",
              "7     I was in a group home when I was first put int...\n",
              "8     I'm proud of you!! Thats a great accomplishmen...\n",
              "9     As a teen I read adult books, murder and horro...\n",
              "10    Someone posted one a while back and it’s still...\n",
              "11    Me and my husband are not having biological ch...\n",
              "12    Have you heard of the organization called A Ho...\n",
              "13    [deleted],I think an it this often. All these ...\n",
              "14    Each person's location is noted at 15-minute i...\n",
              "15    Totally agree.,Our lineage is the Ex fosters w...\n",
              "16    May I ask when this was and how old you are no...\n",
              "17    I’m an former foster youth myself that’s just ...\n",
              "18    I think there were rules at the houses I lived...\n",
              "19    That’s what I was trying to sing lol,If you li...\n",
              "20    I'm also sponsoring the lover of sewing, dysto...\n",
              "21    This Sponsorship program sounds amazing!  Is t...\n",
              "Name: body, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmxfCGlITV0T"
      },
      "source": [
        "#creating smaller dataframe\n",
        "#we do not need all the columns\n",
        "df = df[['id','link_id','body','created','author']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fg6x1_RPTV0W"
      },
      "source": [
        "def first_pass(text): \n",
        "    text = str(text)\n",
        "    #remove newline character code\n",
        "    text = text.replace('\\\\n',' ')\n",
        "    #remove the ampersand character code\n",
        "    text = text.replace('&amp',' ')\n",
        "    #character code for a Zero-witdh space, which is a character that acts like a spacebar, except it's invisible\n",
        "    #https://www.reddit.com/r/OutOfTheLoop/comments/9abjhm/what_does_x200b_mean/\n",
        "    text = text.replace(';#x200B;',' ')\n",
        "    #A commonly used character in HTML is the non-breaking space: &nbsp;\n",
        "    text = text.replace('nbsp',' ')\n",
        "    \n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PfCHH3TTV0Y"
      },
      "source": [
        "df_p_threads['clean_text'] = df_p_threads.body.apply(first_pass)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y17UFbkZbngZ"
      },
      "source": [
        "def second_pass(text): \n",
        "    #make string lowercase \n",
        "    text = str(text)\n",
        "    text = text.lower()\n",
        "    \n",
        "    #remove links\n",
        "    text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
        "\n",
        "    #tokenize\n",
        "    #create bag of words\n",
        "    #1-grams \n",
        "    tokens = nltk.word_tokenize(text) \n",
        "    clean_text = []\n",
        "    \n",
        "    #remove stopwords, puncuation, then lemmatize\n",
        "    for word in tokens:\n",
        "        if (word not in stopwords_english and word not in string.punctuation): \n",
        "            token = wordnet_lemmatizer.lemmatize(word)\n",
        "            clean_text.append(token)\n",
        "            \n",
        "    #remove words of length 3 or smaller        \n",
        "    clean_text = [token for token in clean_text if len(token) > 3] \n",
        "            \n",
        "    return clean_text      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHu8BWyyWxv2"
      },
      "source": [
        "df_p_threads['clean_text'] = df_p_threads.clean_text.apply(second_pass)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1o7cGmnPe3L",
        "outputId": "0998af2a-bb51-4e35-90db-5b9e8245769d"
      },
      "source": [
        "df_p_threads.clean_text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     [thank, teacher, keep, contact, would, love, s...\n",
              "1     [loved, respite, people, stayed, awesome, fost...\n",
              "2     [foster, care, 16-17, feel, always, feel, weir...\n",
              "3     [really, happy, hear, thing, shipped, quickly,...\n",
              "4     [sorry, loss, going, thing, lost, pupper, resc...\n",
              "5     [honestly, know, accustomed, family, dynamic, ...\n",
              "6     [right, systemic, barrier, aged, face, rest, l...\n",
              "7     [group, home, first, care, home, taking, city,...\n",
              "8     [proud, thats, great, accomplishment, foster, ...\n",
              "9     [teen, read, adult, book, murder, horror, alex...\n",
              "10               [someone, posted, back, still, active]\n",
              "11    [husband, biological, child, talked, fostering...\n",
              "12    [heard, organization, called, home, within, co...\n",
              "13    [deleted, think, often, people, agreed, help, ...\n",
              "14    [person, location, noted, 15-minute, interval,...\n",
              "15    [totally, agree., lineage, foster, came, cultu...\n",
              "16    [sorry, went, life, foster, home, freaking, mo...\n",
              "17    [former, foster, youth, trying, good, custom, ...\n",
              "18    [think, rule, house, lived, staff, cared, mana...\n",
              "19                  [trying, sing, like, piña, coladas]\n",
              "20    [also, sponsoring, lover, sewing, dystopian, n...\n",
              "21    [sponsorship, program, sound, amazing, going, ...\n",
              "Name: clean_text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHOYZxQLnTen"
      },
      "source": [
        "docs = df_p_threads.clean_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XL5iBloondgC",
        "outputId": "cb81e798-0edb-466d-8120-e6deb5bbc4bb"
      },
      "source": [
        "docs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     [thank, teacher, keep, contact, would, love, s...\n",
              "1     [loved, respite, people, stayed, awesome, fost...\n",
              "2     [foster, care, 16-17, feel, always, feel, weir...\n",
              "3     [really, happy, hear, thing, shipped, quickly,...\n",
              "4     [sorry, loss, going, thing, lost, pupper, resc...\n",
              "5     [honestly, know, accustomed, family, dynamic, ...\n",
              "6     [right, systemic, barrier, aged, face, rest, l...\n",
              "7     [group, home, first, care, home, taking, city,...\n",
              "8     [proud, thats, great, accomplishment, foster, ...\n",
              "9     [teen, read, adult, book, murder, horror, alex...\n",
              "10               [someone, posted, back, still, active]\n",
              "11    [husband, biological, child, talked, fostering...\n",
              "12    [heard, organization, called, home, within, co...\n",
              "13    [deleted, think, often, people, agreed, help, ...\n",
              "14    [person, location, noted, 15-minute, interval,...\n",
              "15    [totally, agree., lineage, foster, came, cultu...\n",
              "16    [sorry, went, life, foster, home, freaking, mo...\n",
              "17    [former, foster, youth, trying, good, custom, ...\n",
              "18    [think, rule, house, lived, staff, cared, mana...\n",
              "19                  [trying, sing, like, piña, coladas]\n",
              "20    [also, sponsoring, lover, sewing, dystopian, n...\n",
              "21    [sponsorship, program, sound, amazing, going, ...\n",
              "Name: clean_text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBPdSsKw9PId"
      },
      "source": [
        "'''\n",
        "Gensim Word2Vec Parameters\n",
        "**********************\n",
        "\n",
        "min_count = int - Ignores all words with total absolute frequency lower than this - (2, 100)\n",
        "\n",
        "window = int - The maximum distance between the current and predicted word within a sentence. E.g. window words on the left and window words on the left of our target - (2, 10)\n",
        "\n",
        "size = int - Dimensionality of the feature vectors. - (50, 300)\n",
        "\n",
        "sample = float - The threshold for configuring which higher-frequency words are randomly downsampled. Highly influencial. - (0, 1e-5)\n",
        "\n",
        "alpha = float - The initial learning rate - (0.01, 0.05)\n",
        "\n",
        "min_alpha = float - Learning rate will linearly drop to min_alpha as training progresses. To set it: alpha - (min_alpha * epochs) ~ 0.00\n",
        "\n",
        "negative = int - If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\" should be drown. If set to 0, no negative sampling is used. - (5, 20)\n",
        "\n",
        "workers = int - Use these many worker threads to train the model (=faster training with multicore machines)\n",
        "'''\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4J2jtvbTicc"
      },
      "source": [
        "model = Word2Vec(docs, min_count=5,size= 50,workers=3, window =10, sg = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4ZIeXYYTmrM",
        "outputId": "8bd5ea58-3e9e-42cc-9b4f-f9b13c3006a4"
      },
      "source": [
        "model.most_similar('adoption')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('anything', 0.9993597269058228),\n",
              " ('coming', 0.9993193745613098),\n",
              " ('still', 0.9992997646331787),\n",
              " ('honestly', 0.9992932677268982),\n",
              " ('mind', 0.9992665648460388),\n",
              " ('specific', 0.9992642998695374),\n",
              " ('called', 0.9992547631263733),\n",
              " ('give', 0.9992499947547913),\n",
              " ('post', 0.9992194175720215),\n",
              " ('support', 0.9992189407348633)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLF9sWpaToWc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}