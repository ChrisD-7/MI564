{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TFIDF_Intro.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPrdaJGTE6RdU1WugZsES6F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tawfiqam/MI564/blob/main/TFIDF_Intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZEBn77ibrvH"
      },
      "source": [
        "#Revisiting Bag of Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4tpaRancBV2"
      },
      "source": [
        "Let's revisit bag of words (BoA) from [the Naive Bayes classifier example](https://github.com/tawfiqam/MI564/blob/main/Naive_Bayes_Intro.ipynb).\n",
        "\n",
        "\n",
        "The text:\n",
        "\n",
        "`John likes to watch movies. Mary likes movies too. Each key is the word, and each value is the number of occurrences of that word in the given text document.`\n",
        "\n",
        "BoW = {\"John\":1,\"likes\":2,\"to\":1,\"watch\":1,\"movies\":2,\"Mary\":1,\"too\":1}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQN2bu0ScmCM",
        "outputId": "c4e1567a-d335-4085-b15d-2c91e6fdee94"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from gensim import corpora\n",
        "from nltk.corpus import stopwords\n",
        "from collections import defaultdict\n",
        "\n",
        "documents = [\n",
        "    \"Human machine interface for lab abc computer applications\",\n",
        "    \"A survey of user opinion of computer system response time\",\n",
        "    \"The EPS user interface management system\",\n",
        "    \"System and human system engineering testing of EPS\",\n",
        "    \"Relation of user perceived response time to error measurement\",\n",
        "    \"The generation of random binary unordered trees\",\n",
        "    \"The intersection graph of paths in trees\",\n",
        "    \"Graph minors IV Widths of trees and well quasi ordering\",\n",
        "    \"Graph minors A survey\",\n",
        "]\n",
        "\n",
        "stoplist = stopwords.words('english')\n",
        "\n",
        "texts = [\n",
        "    [word for word in document.lower().split() if word not in stoplist]\n",
        "    for document in documents\n",
        "]\n",
        "\n",
        "# remove words that appear only once\n",
        "frequency = defaultdict(int)\n",
        "for text in texts:\n",
        "    for token in text:\n",
        "        frequency[token] += 1\n",
        "\n",
        "texts = [\n",
        "    [token for token in text if frequency[token] > 1]\n",
        "    for text in texts\n",
        "]\n",
        "\n",
        "dictionary = corpora.Dictionary(texts)\n",
        "\n",
        "corpus = [dictionary.doc2bow(text) for text in texts]\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWfoaE2aurpC",
        "outputId": "fe8c0136-92bc-473b-cc6e-1451bae47d54"
      },
      "source": [
        "print(texts)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['human', 'interface', 'computer'], ['survey', 'user', 'computer', 'system', 'response', 'time'], ['eps', 'user', 'interface', 'system'], ['system', 'human', 'system', 'eps'], ['user', 'response', 'time'], ['trees'], ['graph', 'trees'], ['graph', 'minors', 'trees'], ['graph', 'minors', 'survey']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LDD9LmxeXUZ",
        "outputId": "1643b42c-3340-4e26-f4f8-b60b62911cbb"
      },
      "source": [
        "print(\"The dictionary has: \" +str(len(dictionary)) + \" tokens\")\n",
        "\n",
        "for k, v in dictionary.token2id.items():\n",
        "    print(f'{k:{15}} {v:{10}}')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The dictionary has: 12 tokens\n",
            "computer                 0\n",
            "human                    1\n",
            "interface                2\n",
            "response                 3\n",
            "survey                   4\n",
            "system                   5\n",
            "time                     6\n",
            "user                     7\n",
            "eps                      8\n",
            "trees                    9\n",
            "graph                   10\n",
            "minors                  11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lDWM9bee8SV",
        "outputId": "b695c730-bd1e-49cc-8067-9890eb7409c8"
      },
      "source": [
        "#The corpus contains what we call a word vector\n",
        "corpus"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[(0, 1), (1, 1), (2, 1)],\n",
              " [(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)],\n",
              " [(2, 1), (5, 1), (7, 1), (8, 1)],\n",
              " [(1, 1), (5, 2), (8, 1)],\n",
              " [(3, 1), (6, 1), (7, 1)],\n",
              " [(9, 1)],\n",
              " [(9, 1), (10, 1)],\n",
              " [(9, 1), (10, 1), (11, 1)],\n",
              " [(4, 1), (10, 1), (11, 1)]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVVODUWslNFS"
      },
      "source": [
        "    \"System and human system engineering testing of EPS\",\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFYr5hefZoOx"
      },
      "source": [
        "TFIDF is the Term Frequency-Inverse Document Frequency model. Much like count vectorizer introduced in  is also a bag-of-words model. \n",
        "\n",
        "The difference here is that we are weighting the words so that those words that appear more rarely have a higher weight than those that appear at a higher frequency. Words appearing frequently across documents are less important. Those occuring more rarely, but not too rarely, are more important.\n",
        "\n",
        "Then after that at the time of transformation, it takes a vector representation and returns another vector representation. The output vector will have the same dimensionality but the value of the rare features (at the time of training) will be increased. It basically converts integer-valued vectors into real-valued vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKyq-NgFefju",
        "outputId": "a3380f14-ee64-4710-9b12-9027b06dec6a"
      },
      "source": [
        "from gensim.models import TfidfModel\n",
        "from gensim.corpora import Dictionary\n",
        "\n",
        "\n",
        "model = TfidfModel(corpus)\n",
        "\n",
        "topWords = {}\n",
        "\n",
        "corpus_tfidf = model[corpus]\n",
        "\n",
        "for doc in corpus_tfidf:\n",
        "    for iWord, tf_idf in doc:\n",
        "        if iWord not in topWords:\n",
        "            topWords[iWord] = 0\n",
        "\n",
        "        if tf_idf > topWords[iWord]:\n",
        "            topWords[iWord] = tf_idf\n",
        "\n",
        "wordimportance = []\n",
        "for i, item in enumerate(sorted(topWords.items(), key=lambda x: x[1], reverse=True), 1):\n",
        "    wordimportance.append((dictionary[item[0]],item[1]))\n",
        "    print(\"%2s: %-13s %s\" % (i, dictionary[item[0]], item[1]))\n",
        "    if i == 100: break"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 1: trees         1.0\n",
            " 2: system        0.7184811607083769\n",
            " 3: graph         0.7071067811865475\n",
            " 4: minors        0.695546419520037\n",
            " 5: response      0.6282580468670046\n",
            " 6: survey        0.6282580468670046\n",
            " 7: time          0.6282580468670046\n",
            " 8: computer      0.5773502691896257\n",
            " 9: human         0.5773502691896257\n",
            "10: interface     0.5773502691896257\n",
            "11: eps           0.5710059809418182\n",
            "12: user          0.45889394536615247\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2roNlIxWsKLO"
      },
      "source": [
        "from gensim import similarities\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "fcorpus_tfidf = model[corpus]\n",
        "\n",
        "\n",
        "#Query:\n",
        "query_text = \"I love icecream and gensim\"\n",
        "query_text = query_text.lower()\n",
        "query_text = word_tokenize(query_text)\n",
        "vec_bow = dictionary.doc2bow(query_text)\n",
        "vec_tfidf = model[vec_bow]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzCGcpPksTTr",
        "outputId": "cc9f7d32-5e5b-412d-d649-190ca819b252"
      },
      "source": [
        "fcorpus_tfidf"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<gensim.interfaces.TransformedCorpus at 0x7f5651a4b650>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDNT8oQHs7Uq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}