{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TFIDF_Intro.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPGex5vV/Dl19MRqvSn7LJ4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tawfiqam/MI564/blob/main/TFIDF_Intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZEBn77ibrvH"
      },
      "source": [
        "#Revisiting Bag of Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4tpaRancBV2"
      },
      "source": [
        "Let's revisit bag of words (BoA) from [the Naive Bayes classifier example](https://github.com/tawfiqam/MI564/blob/main/Naive_Bayes_Intro.ipynb).\n",
        "\n",
        "\n",
        "The text:\n",
        "\n",
        "`John likes to watch movies. Mary likes movies too. Each key is the word, and each value is the number of occurrences of that word in the given text document.`\n",
        "\n",
        "BoW = {\"John\":1,\"likes\":2,\"to\":1,\"watch\":1,\"movies\":2,\"Mary\":1,\"too\":1}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "HQN2bu0ScmCM",
        "outputId": "ea8a01cc-8677-49f9-828a-5b8250c424bd"
      },
      "source": [
        "#nltk is the natural language toolkit https://www.nltk.org/\n",
        "#it provides a lot of tools needed for NLP analysis\n",
        "import nltk\n",
        "#use nltk to get stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "#gensim is a python library used mostly for NLP analysis\n",
        "#corpora allows us to build a corpus out of different \n",
        "#documents\n",
        "from gensim import corpora\n",
        "#Defaultdict is a container like dictionaries present in the module collections. \n",
        "#Defaultdict is a sub-class of the dict class that returns a dictionary-like object. \n",
        "#The functionality of both dictionaries and defualtdict are almost same except for \n",
        "#the fact that defualtdict never raises a KeyError. It provides a default value \n",
        "#for the key that does not exists.\n",
        "from collections import defaultdict\n",
        "\n",
        "#these are the document samples we are going to use here as an example...\n",
        "documents = [\n",
        "    \"Human machine interface for lab abc computer applications\",\n",
        "    \"A survey of user opinion of computer system response time\",\n",
        "    \"The EPS user interface management system\",\n",
        "    \"System and human system engineering testing of EPS\",\n",
        "    \"Relation of user perceived response time to error measurement\",\n",
        "    \"The generation of random binary unordered trees\",\n",
        "    \"The intersection graph of paths in trees\",\n",
        "    \"Graph minors IV Widths of trees and well quasi ordering\",\n",
        "    \"Graph minors A survey\"\n",
        "]\n",
        "\n",
        "#create a stoplist that includes all the english stop words\n",
        "stoplist = stopwords.words('english')\n",
        "display('here is a subset of the stop words')\n",
        "display(stoplist[0:10])\n",
        "\n",
        "#Now, let's remove words that are not in the stoplist and change all \n",
        "#text to lower case\n",
        "texts = [\n",
        "    [word for word in document.lower().split() if word not in stoplist]\n",
        "    for document in documents\n",
        "]\n",
        "\n",
        "# remove words that appear only once\n",
        "\n",
        "#first, get the frequncy of each word\n",
        "frequency = defaultdict(int)\n",
        "for text in texts:\n",
        "    for token in text:\n",
        "        frequency[token] += 1\n",
        "\n",
        "texts = [\n",
        "    [token for token in text if frequency[token] > 1]\n",
        "    for text in texts\n",
        "]\n",
        "\n",
        "\n",
        "#Next, we'll create a gensim dictionary... \n",
        "dictionary = corpora.Dictionary(texts)\n",
        "\n",
        "#and then we create a corpus of different documents\n",
        "#so, a collection of documents is a corpus (corpora is plural)\n",
        "corpus = [dictionary.doc2bow(text) for text in texts]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'here is a subset of the stop words'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWfoaE2aurpC",
        "outputId": "a3a87eb5-3b6d-4355-9028-b14fe27b2e0f"
      },
      "source": [
        "#Let's take a look at how the tokens look like...\n",
        "print(texts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['human', 'interface', 'computer'], ['survey', 'user', 'computer', 'system', 'response', 'time'], ['eps', 'user', 'interface', 'system'], ['system', 'human', 'system', 'eps'], ['user', 'response', 'time'], ['trees'], ['graph', 'trees'], ['graph', 'minors', 'trees'], ['graph', 'minors', 'survey']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LDD9LmxeXUZ",
        "outputId": "bb43e452-57b0-45e7-eb3f-8e0608eab17f"
      },
      "source": [
        "#Now...recall that we created a gensim dictionary...\n",
        "#Let's see what that looks like...\n",
        "#Notice that each word has a related number, so...\n",
        "#computer --> 0\n",
        "#system   --> 5 [there's a reason I point this one out]\n",
        "print(\"The dictionary has: \" +str(len(dictionary)) + \" tokens\")\n",
        "#token.2id shows the features and the ids associated with them...\n",
        "for k, v in dictionary.token2id.items():\n",
        "    print(f'{k:{15}} {v:{10}}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The dictionary has: 12 tokens\n",
            "computer                 0\n",
            "human                    1\n",
            "interface                2\n",
            "response                 3\n",
            "survey                   4\n",
            "system                   5\n",
            "time                     6\n",
            "user                     7\n",
            "eps                      8\n",
            "trees                    9\n",
            "graph                   10\n",
            "minors                  11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lDWM9bee8SV",
        "outputId": "2e064352-6414-44f4-d907-c19c76231752"
      },
      "source": [
        "#The corpus contains what we call a word vector\n",
        "#so each of the document is now described as a tuple\n",
        "#(token_number,frequency)\n",
        "#take a look @ the fourth row from the top\n",
        "#see how 5,systems is counted twice...why is that \n",
        "#(take a look at the 4th document from the top in our example)\n",
        "#\"System and human system engineering testing of EPS\"\n",
        "corpus"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[(0, 1), (1, 1), (2, 1)],\n",
              " [(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)],\n",
              " [(2, 1), (5, 1), (7, 1), (8, 1)],\n",
              " [(1, 1), (5, 2), (8, 1)],\n",
              " [(3, 1), (6, 1), (7, 1)],\n",
              " [(9, 1)],\n",
              " [(9, 1), (10, 1)],\n",
              " [(9, 1), (10, 1), (11, 1)],\n",
              " [(4, 1), (10, 1), (11, 1)]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVVODUWslNFS"
      },
      "source": [
        "    \"System and human system engineering testing of EPS\",\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFYr5hefZoOx"
      },
      "source": [
        "TFIDF is the Term Frequency-Inverse Document Frequency model. Much like count vectorizer introduced in  is also a bag-of-words model. \n",
        "\n",
        "The difference here is that we are weighting the words so that those words that appear more rarely have a higher weight than those that appear at a higher frequency. Words appearing frequently across documents are less important. Those occuring more rarely, but not too rarely, are more important.\n",
        "\n",
        "Then after that at the time of transformation, it takes a vector representation and returns another vector representation. The output vector will have the same dimensionality but the value of the rare features (at the time of training) will be increased. It basically converts integer-valued vectors into real-valued vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKyq-NgFefju",
        "outputId": "f86fa86d-8699-44a3-c365-d5834f629f95"
      },
      "source": [
        "from gensim.models import TfidfModel\n",
        "from gensim.corpora import Dictionary\n",
        "\n",
        "#now let's fir a TF-IDF model on the corpus\n",
        "model = TfidfModel(corpus) \n",
        "\n",
        "#apply the model to our corpus\n",
        "corpus_tfidf = model[corpus]\n",
        "\n",
        "#creating a dictionary that will hold a word and associated weight\n",
        "topWords = {}\n",
        "\n",
        "for doc in corpus_tfidf:\n",
        "    for iWord, tf_idf in doc:\n",
        "        if iWord not in topWords:\n",
        "            topWords[iWord] = 0\n",
        "\n",
        "        if tf_idf > topWords[iWord]:\n",
        "            topWords[iWord] = tf_idf\n",
        "\n",
        "wordimportance = []\n",
        "for i, item in enumerate(sorted(topWords.items(), key=lambda x: x[1], reverse=True), 1):\n",
        "    wordimportance.append((dictionary[item[0]],item[1]))\n",
        "    print(\"%2s: %-13s %s\" % (i, dictionary[item[0]], item[1]))\n",
        "    if i == 100: break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 1: trees         1.0\n",
            " 2: system        0.7184811607083769\n",
            " 3: graph         0.7071067811865475\n",
            " 4: minors        0.695546419520037\n",
            " 5: response      0.6282580468670046\n",
            " 6: survey        0.6282580468670046\n",
            " 7: time          0.6282580468670046\n",
            " 8: computer      0.5773502691896257\n",
            " 9: human         0.5773502691896257\n",
            "10: interface     0.5773502691896257\n",
            "11: eps           0.5710059809418182\n",
            "12: user          0.45889394536615247\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycPasfBeQs10",
        "outputId": "c26c3f3d-9409-4d4f-a5e7-ec50c0aea44b"
      },
      "source": [
        "model.idfs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 2.1699250014423126,\n",
              " 1: 2.1699250014423126,\n",
              " 2: 2.1699250014423126,\n",
              " 3: 2.1699250014423126,\n",
              " 4: 2.1699250014423126,\n",
              " 5: 1.5849625007211563,\n",
              " 6: 2.1699250014423126,\n",
              " 7: 1.5849625007211563,\n",
              " 8: 2.1699250014423126,\n",
              " 9: 1.5849625007211563,\n",
              " 10: 1.5849625007211563,\n",
              " 11: 2.1699250014423126}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b664MEV6Qi2c",
        "outputId": "eb49a0fb-57b7-4d57-90b8-1c5edfe5d36e"
      },
      "source": [
        "corpus_tfidf.corpus"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[(0, 1), (1, 1), (2, 1)],\n",
              " [(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)],\n",
              " [(2, 1), (5, 1), (7, 1), (8, 1)],\n",
              " [(1, 1), (5, 2), (8, 1)],\n",
              " [(3, 1), (6, 1), (7, 1)],\n",
              " [(9, 1)],\n",
              " [(9, 1), (10, 1)],\n",
              " [(9, 1), (10, 1), (11, 1)],\n",
              " [(4, 1), (10, 1), (11, 1)]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ak1eK7KLBxm9"
      },
      "source": [
        "By creating a TF-IDF "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2roNlIxWsKLO",
        "outputId": "9a81aee5-0a20-4add-80f4-0dadec5db08c"
      },
      "source": [
        "!pip install psaw"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting psaw\n",
            "  Downloading https://files.pythonhosted.org/packages/01/fe/e2f43241ff7545588d07bb93dd353e4333ebc02c31d7e0dc36a8a9d93214/psaw-0.1.0-py3-none-any.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from psaw) (2.23.0)\n",
            "Requirement already satisfied: Click in /usr/local/lib/python3.7/dist-packages (from psaw) (7.1.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->psaw) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->psaw) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->psaw) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->psaw) (2.10)\n",
            "Installing collected packages: psaw\n",
            "Successfully installed psaw-0.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzCGcpPksTTr"
      },
      "source": [
        "import pandas as pd\n",
        "#we will need datetime in order to specify the timeline we need to collect the data\n",
        "import datetime as dt\n",
        "\n",
        "#now we import the wrapper in order to use the API\n",
        "from psaw import PushshiftAPI\n",
        "\n",
        "api = PushshiftAPI()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDNT8oQHs7Uq"
      },
      "source": [
        "#this function will allow us to find the last day of each month\n",
        "#for example, there are 31 days in January, but 28 this February\n",
        "def last_day_of_month(any_day):\n",
        "    # this will never fail\n",
        "    # get close to the end of the month for any day, and add 4 days 'over'\n",
        "    next_month = any_day.replace(day=28) + datetime.timedelta(days=4)\n",
        "    # subtract the number of remaining 'overage' days to get last day of current month, or said programattically said, the previous day of the first of next month\n",
        "    return next_month - datetime.timedelta(days=next_month.day)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5uVu-Sz5I3iL",
        "outputId": "f48c55ae-2b3a-4162-dece-4da639ed7ba3"
      },
      "source": [
        "\n",
        "\n",
        "import datetime\n",
        "subredditlist = ['Ex_Foster']\n",
        "for reddit in subredditlist:\n",
        "    for y in range(2019,2021):\n",
        "      for i in range(1,12):\n",
        "          file_name= str(reddit)+\"_\"+str(y)+\"_\"+str(i)+\".json\"\n",
        "          print(\"starting with the month \"+str(i))\n",
        "          print(\"for subreddit...\"+str(reddit))\n",
        "          print(\"setting start epoch...\")\n",
        "          start_epoch=int(dt.datetime(y, i, 1).timestamp())\n",
        "          print(\"setting end epoch...\")\n",
        "          last_day = last_day_of_month(datetime.date(y, i, 1))\n",
        "          print(\"the last day of the month is...\")\n",
        "          print(last_day.day)\n",
        "          last_day = int(last_day.day)\n",
        "          end_epoch = int(dt.datetime(y,i,last_day).timestamp())\n",
        "          print(\"setting up the generator...\")\n",
        "          gen = api.search_comments(after=start_epoch, before=end_epoch,subreddit=reddit)\n",
        "          print(\"setting up the dataframe...\")\n",
        "          df = pd.DataFrame([obj.d_ for obj in gen])\n",
        "          print(\"The number of comments for year \"+ str(y)+\" and month \"+str(i)+\" is \"+str(len(df.index)))\n",
        "          df.to_json(file_name)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "starting with the month 1\n",
            "for subreddit...Ex_Foster\n",
            "setting start epoch...\n",
            "setting end epoch...\n",
            "the last day of the month is...\n",
            "31\n",
            "setting up the generator...\n",
            "setting up the dataframe...\n",
            "The number of comments for year 2019 and month 1 is 0\n",
            "starting with the month 2\n",
            "for subreddit...Ex_Foster\n",
            "setting start epoch...\n",
            "setting end epoch...\n",
            "the last day of the month is...\n",
            "28\n",
            "setting up the generator...\n",
            "setting up the dataframe...\n",
            "The number of comments for year 2019 and month 2 is 0\n",
            "starting with the month 3\n",
            "for subreddit...Ex_Foster\n",
            "setting start epoch...\n",
            "setting end epoch...\n",
            "the last day of the month is...\n",
            "31\n",
            "setting up the generator...\n",
            "setting up the dataframe...\n",
            "The number of comments for year 2019 and month 3 is 69\n",
            "starting with the month 4\n",
            "for subreddit...Ex_Foster\n",
            "setting start epoch...\n",
            "setting end epoch...\n",
            "the last day of the month is...\n",
            "30\n",
            "setting up the generator...\n",
            "setting up the dataframe...\n",
            "The number of comments for year 2019 and month 4 is 233\n",
            "starting with the month 5\n",
            "for subreddit...Ex_Foster\n",
            "setting start epoch...\n",
            "setting end epoch...\n",
            "the last day of the month is...\n",
            "31\n",
            "setting up the generator...\n",
            "setting up the dataframe...\n",
            "The number of comments for year 2019 and month 5 is 119\n",
            "starting with the month 6\n",
            "for subreddit...Ex_Foster\n",
            "setting start epoch...\n",
            "setting end epoch...\n",
            "the last day of the month is...\n",
            "30\n",
            "setting up the generator...\n",
            "setting up the dataframe...\n",
            "The number of comments for year 2019 and month 6 is 183\n",
            "starting with the month 7\n",
            "for subreddit...Ex_Foster\n",
            "setting start epoch...\n",
            "setting end epoch...\n",
            "the last day of the month is...\n",
            "31\n",
            "setting up the generator...\n",
            "setting up the dataframe...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/psaw/PushshiftAPI.py:192: UserWarning: Got non 200 code 429\n",
            "  warnings.warn(\"Got non 200 code %s\" % response.status_code)\n",
            "/usr/local/lib/python3.7/dist-packages/psaw/PushshiftAPI.py:180: UserWarning: Unable to connect to pushshift.io. Retrying after backoff.\n",
            "  warnings.warn(\"Unable to connect to pushshift.io. Retrying after backoff.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The number of comments for year 2019 and month 7 is 78\n",
            "starting with the month 8\n",
            "for subreddit...Ex_Foster\n",
            "setting start epoch...\n",
            "setting end epoch...\n",
            "the last day of the month is...\n",
            "31\n",
            "setting up the generator...\n",
            "setting up the dataframe...\n",
            "The number of comments for year 2019 and month 8 is 80\n",
            "starting with the month 9\n",
            "for subreddit...Ex_Foster\n",
            "setting start epoch...\n",
            "setting end epoch...\n",
            "the last day of the month is...\n",
            "30\n",
            "setting up the generator...\n",
            "setting up the dataframe...\n",
            "The number of comments for year 2019 and month 9 is 229\n",
            "starting with the month 10\n",
            "for subreddit...Ex_Foster\n",
            "setting start epoch...\n",
            "setting end epoch...\n",
            "the last day of the month is...\n",
            "31\n",
            "setting up the generator...\n",
            "setting up the dataframe...\n",
            "The number of comments for year 2019 and month 10 is 351\n",
            "starting with the month 11\n",
            "for subreddit...Ex_Foster\n",
            "setting start epoch...\n",
            "setting end epoch...\n",
            "the last day of the month is...\n",
            "30\n",
            "setting up the generator...\n",
            "setting up the dataframe...\n",
            "The number of comments for year 2019 and month 11 is 388\n",
            "starting with the month 1\n",
            "for subreddit...Ex_Foster\n",
            "setting start epoch...\n",
            "setting end epoch...\n",
            "the last day of the month is...\n",
            "31\n",
            "setting up the generator...\n",
            "setting up the dataframe...\n",
            "The number of comments for year 2020 and month 1 is 134\n",
            "starting with the month 2\n",
            "for subreddit...Ex_Foster\n",
            "setting start epoch...\n",
            "setting end epoch...\n",
            "the last day of the month is...\n",
            "29\n",
            "setting up the generator...\n",
            "setting up the dataframe...\n",
            "The number of comments for year 2020 and month 2 is 210\n",
            "starting with the month 3\n",
            "for subreddit...Ex_Foster\n",
            "setting start epoch...\n",
            "setting end epoch...\n",
            "the last day of the month is...\n",
            "31\n",
            "setting up the generator...\n",
            "setting up the dataframe...\n",
            "The number of comments for year 2020 and month 3 is 275\n",
            "starting with the month 4\n",
            "for subreddit...Ex_Foster\n",
            "setting start epoch...\n",
            "setting end epoch...\n",
            "the last day of the month is...\n",
            "30\n",
            "setting up the generator...\n",
            "setting up the dataframe...\n",
            "The number of comments for year 2020 and month 4 is 279\n",
            "starting with the month 5\n",
            "for subreddit...Ex_Foster\n",
            "setting start epoch...\n",
            "setting end epoch...\n",
            "the last day of the month is...\n",
            "31\n",
            "setting up the generator...\n",
            "setting up the dataframe...\n",
            "The number of comments for year 2020 and month 5 is 501\n",
            "starting with the month 6\n",
            "for subreddit...Ex_Foster\n",
            "setting start epoch...\n",
            "setting end epoch...\n",
            "the last day of the month is...\n",
            "30\n",
            "setting up the generator...\n",
            "setting up the dataframe...\n",
            "The number of comments for year 2020 and month 6 is 185\n",
            "starting with the month 7\n",
            "for subreddit...Ex_Foster\n",
            "setting start epoch...\n",
            "setting end epoch...\n",
            "the last day of the month is...\n",
            "31\n",
            "setting up the generator...\n",
            "setting up the dataframe...\n",
            "The number of comments for year 2020 and month 7 is 353\n",
            "starting with the month 8\n",
            "for subreddit...Ex_Foster\n",
            "setting start epoch...\n",
            "setting end epoch...\n",
            "the last day of the month is...\n",
            "31\n",
            "setting up the generator...\n",
            "setting up the dataframe...\n",
            "The number of comments for year 2020 and month 8 is 217\n",
            "starting with the month 9\n",
            "for subreddit...Ex_Foster\n",
            "setting start epoch...\n",
            "setting end epoch...\n",
            "the last day of the month is...\n",
            "30\n",
            "setting up the generator...\n",
            "setting up the dataframe...\n",
            "The number of comments for year 2020 and month 9 is 217\n",
            "starting with the month 10\n",
            "for subreddit...Ex_Foster\n",
            "setting start epoch...\n",
            "setting end epoch...\n",
            "the last day of the month is...\n",
            "31\n",
            "setting up the generator...\n",
            "setting up the dataframe...\n",
            "The number of comments for year 2020 and month 10 is 222\n",
            "starting with the month 11\n",
            "for subreddit...Ex_Foster\n",
            "setting start epoch...\n",
            "setting end epoch...\n",
            "the last day of the month is...\n",
            "30\n",
            "setting up the generator...\n",
            "setting up the dataframe...\n",
            "The number of comments for year 2020 and month 11 is 100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNBcWMbjJITl"
      },
      "source": [
        "#this function from sklearn allow us to create a TFIDF vector for our text\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from collections import defaultdict\n",
        "\n",
        "#We'll create a vector here, and as we did before use the stop words provided by sklearn\n",
        "#Now notice the ngram range. The reason we use the word ngram is that the n can be changed! \n",
        "#So, for example, 1-gram is a one word...2-grams is a collection of two words...etc...\n",
        "#In this example, we're creating a collection that includes between 1 and 3 grams\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1,3),stop_words=stoplist)\n",
        "#Fitting the model\n",
        "X = vectorizer.fit_transform(df['body'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQBUwejMKLeW"
      },
      "source": [
        "#since we have 3 grams (1,2,3), we need to create a dictionary for each\n",
        "features_by_gram = defaultdict(list)\n",
        "\n",
        "#zip is a helpful function in python  that aggregates iteratbles in a tuple (list1,list2)\n",
        "#list one in this case are all the features in the mode, and idf_ The inverse document frequency (IDF) vector\n",
        "for f, w in zip(vectorizer.get_feature_names(), vectorizer.idf_):\n",
        "    features_by_gram[len(f.split(' '))].append((f, w))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mN1TkSA3KOd2",
        "outputId": "895d8960-118b-48c3-e3a9-c0173c41acfa"
      },
      "source": [
        "#lets get the top tokens for each of the grams\n",
        "top_n = 20\n",
        "\n",
        "#for each of the grams, we want to get the top features\n",
        "for gram, features in features_by_gram.items():\n",
        "    #get sorted top features\n",
        "    top_features = sorted(features, key=lambda x: x[1], reverse=False)[:top_n]\n",
        "    #get the top features and their weights\n",
        "    #now just focus on getting the first element of the tuple\n",
        "    top_features = [f[0] for f in top_features]\n",
        "    print('{}-gram top:'.format(gram), top_features)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1-gram top: ['like', 'like', 'know', 'sorry', 'know', 'sorry', 'people', 'people', 'family', 'going', 'life', 'family', 'going', 'life', 'good', 'want', 'good', 'want', 'care', 'foster']\n",
            "2-gram top: ['foster care', 'sounds like', 'foster care', 'sounds like', 'even though', 'put username', 'sorry loss', 'even though', 'put username', 'sorry loss', 'feel free', 'feels like', 'feel free', 'feels like', 'easier said', 'feel like', 'former foster', 'foster youth', 'go back', 'people know']\n",
            "3-gram top: ['easier said done', 'easier said done', 'ever need someone', 'feel free dm', 'one year care', 'ever need someone', 'feel free dm', 'one year care', '10 years like', '10x definitely blessing', '12 best answers', '13 years hurts', '14 imagining future', '16 especially home', '16 matter wonderful', '18 held resentment', '18 moved another', '20 years ago', '20 years every', '20s man felt']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 744
        },
        "id": "tvk8hpRpKxpx",
        "outputId": "821cece1-e4d1-4706-de44-e49295da2ca0"
      },
      "source": [
        "#lets get the top tokens for each of the grams\n",
        "top_n = 20\n",
        "\n",
        "#for each of the grams, we want to get the top features\n",
        "for gram, features in features_by_gram.items():\n",
        "    #get sorted top features\n",
        "    top_features = sorted(features, key=lambda x: x[1], reverse=False)[:top_n]\n",
        "    #get the top features and their weights\n",
        "    display(top_features[0:5])\n",
        "    display(50*'*')\n",
        "    #now just focus on getting the first element of the tuple\n",
        "    top_features = [f[0] for f in top_features]\n",
        "    display(top_features[0:5])\n",
        "    display(50*'*')\n",
        "    print('{}-gram top:'.format(gram), top_features)\n",
        "    display(50*'*')\n",
        "    display(50*'*')\n",
        "    display(50*'*')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[('like', 2.2139231351791038),\n",
              " ('like', 2.2139231351791038),\n",
              " ('know', 2.570598079117836),\n",
              " ('sorry', 2.570598079117836),\n",
              " ('know', 2.570598079117836)]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'**************************************************'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "['like', 'like', 'know', 'sorry', 'know']"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'**************************************************'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "1-gram top: ['like', 'like', 'know', 'sorry', 'know', 'sorry', 'people', 'people', 'family', 'going', 'life', 'family', 'going', 'life', 'good', 'want', 'good', 'want', 'care', 'foster']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'**************************************************'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'**************************************************'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'**************************************************'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[('foster care', 3.5356789751614235),\n",
              " ('sounds like', 3.5356789751614235),\n",
              " ('foster care', 3.5356789751614235),\n",
              " ('sounds like', 3.5356789751614235),\n",
              " ('even though', 3.8233610476132043)]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'**************************************************'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "['foster care', 'sounds like', 'foster care', 'sounds like', 'even though']"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'**************************************************'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "2-gram top: ['foster care', 'sounds like', 'foster care', 'sounds like', 'even though', 'put username', 'sorry loss', 'even though', 'put username', 'sorry loss', 'feel free', 'feels like', 'feel free', 'feels like', 'easier said', 'feel like', 'former foster', 'foster youth', 'go back', 'people know']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'**************************************************'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'**************************************************'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'**************************************************'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[('easier said done', 4.228826155721369),\n",
              " ('easier said done', 4.228826155721369),\n",
              " ('ever need someone', 4.51650822817315),\n",
              " ('feel free dm', 4.51650822817315),\n",
              " ('one year care', 4.51650822817315)]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'**************************************************'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "['easier said done',\n",
              " 'easier said done',\n",
              " 'ever need someone',\n",
              " 'feel free dm',\n",
              " 'one year care']"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'**************************************************'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "3-gram top: ['easier said done', 'easier said done', 'ever need someone', 'feel free dm', 'one year care', 'ever need someone', 'feel free dm', 'one year care', '10 years like', '10x definitely blessing', '12 best answers', '13 years hurts', '14 imagining future', '16 especially home', '16 matter wonderful', '18 held resentment', '18 moved another', '20 years ago', '20 years every', '20s man felt']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'**************************************************'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'**************************************************'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'**************************************************'"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}