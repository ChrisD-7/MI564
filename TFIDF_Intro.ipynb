{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TFIDF_Intro.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOPbDpFKKBOIGp7D/SQRfEG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tawfiqam/MI564/blob/main/TFIDF_Intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZEBn77ibrvH"
      },
      "source": [
        "#Revisiting Bag of Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4tpaRancBV2"
      },
      "source": [
        "Let's revisit bag of words (BoA) from [the Naive Bayes classifier example](https://github.com/tawfiqam/MI564/blob/main/Naive_Bayes_Intro.ipynb).\n",
        "\n",
        "\n",
        "The text:\n",
        "\n",
        "`John likes to watch movies. Mary likes movies too. Each key is the word, and each value is the number of occurrences of that word in the given text document.`\n",
        "\n",
        "BoW = {\"John\":1,\"likes\":2,\"to\":1,\"watch\":1,\"movies\":2,\"Mary\":1,\"too\":1}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQN2bu0ScmCM",
        "outputId": "63d32260-d443-430f-cad1-17b32396a1b8"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from gensim import corpora\n",
        "from nltk.corpus import stopwords\n",
        "from collections import defaultdict\n",
        "\n",
        "documents = [\n",
        "    \"Human machine interface for lab abc computer applications\",\n",
        "    \"A survey of user opinion of computer system response time\",\n",
        "    \"The EPS user interface management system\",\n",
        "    \"System and human system engineering testing of EPS\",\n",
        "    \"Relation of user perceived response time to error measurement\",\n",
        "    \"The generation of random binary unordered trees\",\n",
        "    \"The intersection graph of paths in trees\",\n",
        "    \"Graph minors IV Widths of trees and well quasi ordering\",\n",
        "    \"Graph minors A survey\"\n",
        "]\n",
        "\n",
        "stoplist = stopwords.words('english')\n",
        "\n",
        "texts = [\n",
        "    [word for word in document.lower().split() if word not in stoplist]\n",
        "    for document in documents\n",
        "]\n",
        "\n",
        "# remove words that appear only once\n",
        "frequency = defaultdict(int)\n",
        "for text in texts:\n",
        "    for token in text:\n",
        "        frequency[token] += 1\n",
        "\n",
        "texts = [\n",
        "    [token for token in text if frequency[token] > 1]\n",
        "    for text in texts\n",
        "]\n",
        "\n",
        "dictionary = corpora.Dictionary(texts)\n",
        "\n",
        "corpus = [dictionary.doc2bow(text) for text in texts]\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWfoaE2aurpC",
        "outputId": "a3a87eb5-3b6d-4355-9028-b14fe27b2e0f"
      },
      "source": [
        "print(texts)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['human', 'interface', 'computer'], ['survey', 'user', 'computer', 'system', 'response', 'time'], ['eps', 'user', 'interface', 'system'], ['system', 'human', 'system', 'eps'], ['user', 'response', 'time'], ['trees'], ['graph', 'trees'], ['graph', 'minors', 'trees'], ['graph', 'minors', 'survey']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LDD9LmxeXUZ",
        "outputId": "2d3a853d-731f-42c3-db34-13671a933d01"
      },
      "source": [
        "print(\"The dictionary has: \" +str(len(dictionary)) + \" tokens\")\n",
        "\n",
        "for k, v in dictionary.token2id.items():\n",
        "    print(f'{k:{15}} {v:{10}}')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The dictionary has: 12 tokens\n",
            "computer                 0\n",
            "human                    1\n",
            "interface                2\n",
            "response                 3\n",
            "survey                   4\n",
            "system                   5\n",
            "time                     6\n",
            "user                     7\n",
            "eps                      8\n",
            "trees                    9\n",
            "graph                   10\n",
            "minors                  11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lDWM9bee8SV",
        "outputId": "2e064352-6414-44f4-d907-c19c76231752"
      },
      "source": [
        "#The corpus contains what we call a word vector\n",
        "corpus"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[(0, 1), (1, 1), (2, 1)],\n",
              " [(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)],\n",
              " [(2, 1), (5, 1), (7, 1), (8, 1)],\n",
              " [(1, 1), (5, 2), (8, 1)],\n",
              " [(3, 1), (6, 1), (7, 1)],\n",
              " [(9, 1)],\n",
              " [(9, 1), (10, 1)],\n",
              " [(9, 1), (10, 1), (11, 1)],\n",
              " [(4, 1), (10, 1), (11, 1)]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVVODUWslNFS"
      },
      "source": [
        "    \"System and human system engineering testing of EPS\",\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFYr5hefZoOx"
      },
      "source": [
        "TFIDF is the Term Frequency-Inverse Document Frequency model. Much like count vectorizer introduced in  is also a bag-of-words model. \n",
        "\n",
        "The difference here is that we are weighting the words so that those words that appear more rarely have a higher weight than those that appear at a higher frequency. Words appearing frequently across documents are less important. Those occuring more rarely, but not too rarely, are more important.\n",
        "\n",
        "Then after that at the time of transformation, it takes a vector representation and returns another vector representation. The output vector will have the same dimensionality but the value of the rare features (at the time of training) will be increased. It basically converts integer-valued vectors into real-valued vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKyq-NgFefju",
        "outputId": "f86fa86d-8699-44a3-c365-d5834f629f95"
      },
      "source": [
        "from gensim.models import TfidfModel\n",
        "from gensim.corpora import Dictionary\n",
        "\n",
        "\n",
        "model = TfidfModel(corpus)\n",
        "\n",
        "topWords = {}\n",
        "\n",
        "corpus_tfidf = model[corpus]\n",
        "\n",
        "for doc in corpus_tfidf:\n",
        "    for iWord, tf_idf in doc:\n",
        "        if iWord not in topWords:\n",
        "            topWords[iWord] = 0\n",
        "\n",
        "        if tf_idf > topWords[iWord]:\n",
        "            topWords[iWord] = tf_idf\n",
        "\n",
        "wordimportance = []\n",
        "for i, item in enumerate(sorted(topWords.items(), key=lambda x: x[1], reverse=True), 1):\n",
        "    wordimportance.append((dictionary[item[0]],item[1]))\n",
        "    print(\"%2s: %-13s %s\" % (i, dictionary[item[0]], item[1]))\n",
        "    if i == 100: break"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 1: trees         1.0\n",
            " 2: system        0.7184811607083769\n",
            " 3: graph         0.7071067811865475\n",
            " 4: minors        0.695546419520037\n",
            " 5: response      0.6282580468670046\n",
            " 6: survey        0.6282580468670046\n",
            " 7: time          0.6282580468670046\n",
            " 8: computer      0.5773502691896257\n",
            " 9: human         0.5773502691896257\n",
            "10: interface     0.5773502691896257\n",
            "11: eps           0.5710059809418182\n",
            "12: user          0.45889394536615247\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ak1eK7KLBxm9"
      },
      "source": [
        "By creating a TF-IDF "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2roNlIxWsKLO",
        "outputId": "9a81aee5-0a20-4add-80f4-0dadec5db08c"
      },
      "source": [
        "!pip install psaw"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting psaw\n",
            "  Downloading https://files.pythonhosted.org/packages/01/fe/e2f43241ff7545588d07bb93dd353e4333ebc02c31d7e0dc36a8a9d93214/psaw-0.1.0-py3-none-any.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from psaw) (2.23.0)\n",
            "Requirement already satisfied: Click in /usr/local/lib/python3.7/dist-packages (from psaw) (7.1.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->psaw) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->psaw) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->psaw) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->psaw) (2.10)\n",
            "Installing collected packages: psaw\n",
            "Successfully installed psaw-0.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzCGcpPksTTr"
      },
      "source": [
        "import pandas as pd\n",
        "#we will need datetime in order to specify the timeline we need to collect the data\n",
        "import datetime as dt\n",
        "\n",
        "#now we import the wrapper in order to use the API\n",
        "from psaw import PushshiftAPI\n",
        "\n",
        "api = PushshiftAPI()"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDNT8oQHs7Uq"
      },
      "source": [
        "#this function will allow us to find the last day of each month\n",
        "#for example, there are 31 days in January, but 28 this February\n",
        "def last_day_of_month(any_day):\n",
        "    # this will never fail\n",
        "    # get close to the end of the month for any day, and add 4 days 'over'\n",
        "    next_month = any_day.replace(day=28) + datetime.timedelta(days=4)\n",
        "    # subtract the number of remaining 'overage' days to get last day of current month, or said programattically said, the previous day of the first of next month\n",
        "    return next_month - datetime.timedelta(days=next_month.day)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5uVu-Sz5I3iL",
        "outputId": "f48c55ae-2b3a-4162-dece-4da639ed7ba3"
      },
      "source": [
        "\n",
        "\n",
        "import datetime\n",
        "subredditlist = ['Ex_Foster']\n",
        "for reddit in subredditlist:\n",
        "    for y in range(2019,2021):\n",
        "      for i in range(1,12):\n",
        "          file_name= str(reddit)+\"_\"+str(y)+\"_\"+str(i)+\".json\"\n",
        "          print(\"starting with the month \"+str(i))\n",
        "          print(\"for subreddit...\"+str(reddit))\n",
        "          print(\"setting start epoch...\")\n",
        "          start_epoch=int(dt.datetime(y, i, 1).timestamp())\n",
        "          print(\"setting end epoch...\")\n",
        "          last_day = last_day_of_month(datetime.date(y, i, 1))\n",
        "          print(\"the last day of the month is...\")\n",
        "          print(last_day.day)\n",
        "          last_day = int(last_day.day)\n",
        "          end_epoch = int(dt.datetime(y,i,last_day).timestamp())\n",
        "          print(\"setting up the generator...\")\n",
        "          gen = api.search_comments(after=start_epoch, before=end_epoch,subreddit=reddit)\n",
        "          print(\"setting up the dataframe...\")\n",
        "          df = pd.DataFrame([obj.d_ for obj in gen])\n",
        "          print(\"The number of comments for year \"+ str(y)+\" and month \"+str(i)+\" is \"+str(len(df.index)))\n",
        "          df.to_json(file_name)\n",
        "\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "starting with the month 1\n",
            "for subreddit...Ex_Foster\n",
            "setting start epoch...\n",
            "setting end epoch...\n",
            "the last day of the month is...\n",
            "31\n",
            "setting up the generator...\n",
            "setting up the dataframe...\n",
            "The number of comments for year 2019 and month 1 is 0\n",
            "starting with the month 2\n",
            "for subreddit...Ex_Foster\n",
            "setting start epoch...\n",
            "setting end epoch...\n",
            "the last day of the month is...\n",
            "28\n",
            "setting up the generator...\n",
            "setting up the dataframe...\n",
            "The number of comments for year 2019 and month 2 is 0\n",
            "starting with the month 3\n",
            "for subreddit...Ex_Foster\n",
            "setting start epoch...\n",
            "setting end epoch...\n",
            "the last day of the month is...\n",
            "31\n",
            "setting up the generator...\n",
            "setting up the dataframe...\n",
            "The number of comments for year 2019 and month 3 is 69\n",
            "starting with the month 4\n",
            "for subreddit...Ex_Foster\n",
            "setting start epoch...\n",
            "setting end epoch...\n",
            "the last day of the month is...\n",
            "30\n",
            "setting up the generator...\n",
            "setting up the dataframe...\n",
            "The number of comments for year 2019 and month 4 is 233\n",
            "starting with the month 5\n",
            "for subreddit...Ex_Foster\n",
            "setting start epoch...\n",
            "setting end epoch...\n",
            "the last day of the month is...\n",
            "31\n",
            "setting up the generator...\n",
            "setting up the dataframe...\n",
            "The number of comments for year 2019 and month 5 is 119\n",
            "starting with the month 6\n",
            "for subreddit...Ex_Foster\n",
            "setting start epoch...\n",
            "setting end epoch...\n",
            "the last day of the month is...\n",
            "30\n",
            "setting up the generator...\n",
            "setting up the dataframe...\n",
            "The number of comments for year 2019 and month 6 is 183\n",
            "starting with the month 7\n",
            "for subreddit...Ex_Foster\n",
            "setting start epoch...\n",
            "setting end epoch...\n",
            "the last day of the month is...\n",
            "31\n",
            "setting up the generator...\n",
            "setting up the dataframe...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/psaw/PushshiftAPI.py:192: UserWarning: Got non 200 code 429\n",
            "  warnings.warn(\"Got non 200 code %s\" % response.status_code)\n",
            "/usr/local/lib/python3.7/dist-packages/psaw/PushshiftAPI.py:180: UserWarning: Unable to connect to pushshift.io. Retrying after backoff.\n",
            "  warnings.warn(\"Unable to connect to pushshift.io. Retrying after backoff.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The number of comments for year 2019 and month 7 is 78\n",
            "starting with the month 8\n",
            "for subreddit...Ex_Foster\n",
            "setting start epoch...\n",
            "setting end epoch...\n",
            "the last day of the month is...\n",
            "31\n",
            "setting up the generator...\n",
            "setting up the dataframe...\n",
            "The number of comments for year 2019 and month 8 is 80\n",
            "starting with the month 9\n",
            "for subreddit...Ex_Foster\n",
            "setting start epoch...\n",
            "setting end epoch...\n",
            "the last day of the month is...\n",
            "30\n",
            "setting up the generator...\n",
            "setting up the dataframe...\n",
            "The number of comments for year 2019 and month 9 is 229\n",
            "starting with the month 10\n",
            "for subreddit...Ex_Foster\n",
            "setting start epoch...\n",
            "setting end epoch...\n",
            "the last day of the month is...\n",
            "31\n",
            "setting up the generator...\n",
            "setting up the dataframe...\n",
            "The number of comments for year 2019 and month 10 is 351\n",
            "starting with the month 11\n",
            "for subreddit...Ex_Foster\n",
            "setting start epoch...\n",
            "setting end epoch...\n",
            "the last day of the month is...\n",
            "30\n",
            "setting up the generator...\n",
            "setting up the dataframe...\n",
            "The number of comments for year 2019 and month 11 is 388\n",
            "starting with the month 1\n",
            "for subreddit...Ex_Foster\n",
            "setting start epoch...\n",
            "setting end epoch...\n",
            "the last day of the month is...\n",
            "31\n",
            "setting up the generator...\n",
            "setting up the dataframe...\n",
            "The number of comments for year 2020 and month 1 is 134\n",
            "starting with the month 2\n",
            "for subreddit...Ex_Foster\n",
            "setting start epoch...\n",
            "setting end epoch...\n",
            "the last day of the month is...\n",
            "29\n",
            "setting up the generator...\n",
            "setting up the dataframe...\n",
            "The number of comments for year 2020 and month 2 is 210\n",
            "starting with the month 3\n",
            "for subreddit...Ex_Foster\n",
            "setting start epoch...\n",
            "setting end epoch...\n",
            "the last day of the month is...\n",
            "31\n",
            "setting up the generator...\n",
            "setting up the dataframe...\n",
            "The number of comments for year 2020 and month 3 is 275\n",
            "starting with the month 4\n",
            "for subreddit...Ex_Foster\n",
            "setting start epoch...\n",
            "setting end epoch...\n",
            "the last day of the month is...\n",
            "30\n",
            "setting up the generator...\n",
            "setting up the dataframe...\n",
            "The number of comments for year 2020 and month 4 is 279\n",
            "starting with the month 5\n",
            "for subreddit...Ex_Foster\n",
            "setting start epoch...\n",
            "setting end epoch...\n",
            "the last day of the month is...\n",
            "31\n",
            "setting up the generator...\n",
            "setting up the dataframe...\n",
            "The number of comments for year 2020 and month 5 is 501\n",
            "starting with the month 6\n",
            "for subreddit...Ex_Foster\n",
            "setting start epoch...\n",
            "setting end epoch...\n",
            "the last day of the month is...\n",
            "30\n",
            "setting up the generator...\n",
            "setting up the dataframe...\n",
            "The number of comments for year 2020 and month 6 is 185\n",
            "starting with the month 7\n",
            "for subreddit...Ex_Foster\n",
            "setting start epoch...\n",
            "setting end epoch...\n",
            "the last day of the month is...\n",
            "31\n",
            "setting up the generator...\n",
            "setting up the dataframe...\n",
            "The number of comments for year 2020 and month 7 is 353\n",
            "starting with the month 8\n",
            "for subreddit...Ex_Foster\n",
            "setting start epoch...\n",
            "setting end epoch...\n",
            "the last day of the month is...\n",
            "31\n",
            "setting up the generator...\n",
            "setting up the dataframe...\n",
            "The number of comments for year 2020 and month 8 is 217\n",
            "starting with the month 9\n",
            "for subreddit...Ex_Foster\n",
            "setting start epoch...\n",
            "setting end epoch...\n",
            "the last day of the month is...\n",
            "30\n",
            "setting up the generator...\n",
            "setting up the dataframe...\n",
            "The number of comments for year 2020 and month 9 is 217\n",
            "starting with the month 10\n",
            "for subreddit...Ex_Foster\n",
            "setting start epoch...\n",
            "setting end epoch...\n",
            "the last day of the month is...\n",
            "31\n",
            "setting up the generator...\n",
            "setting up the dataframe...\n",
            "The number of comments for year 2020 and month 10 is 222\n",
            "starting with the month 11\n",
            "for subreddit...Ex_Foster\n",
            "setting start epoch...\n",
            "setting end epoch...\n",
            "the last day of the month is...\n",
            "30\n",
            "setting up the generator...\n",
            "setting up the dataframe...\n",
            "The number of comments for year 2020 and month 11 is 100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNBcWMbjJITl"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from collections import defaultdict\n",
        "\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1,3),stop_words=stoplist)\n",
        "X = vectorizer.fit_transform(df['body'])\n",
        "features_by_gram = defaultdict(list)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQBUwejMKLeW",
        "outputId": "0610375e-b995-4679-eefc-a9364e09713f"
      },
      "source": [
        "for f, w in zip(vectorizer.get_feature_names(), vectorizer.idf_):\n",
        "    features_by_gram[len(f.split(' '))].append((f, w))\n",
        "\n",
        "top_n = 20\n",
        "for gram, features in features_by_gram.items():\n",
        "    top_features = sorted(features, key=lambda x: x[1], reverse=False)[:top_n]\n",
        "    top_features = [f[0] for f in top_features]\n",
        "    print('{}-gram top:'.format(gram), top_features)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1-gram top: ['like', 'know', 'sorry', 'people', 'family', 'going', 'life', 'good', 'want', 'care', 'foster', 'really', 'say', 'better', 'even', 'love', 'things', 'feel', 'get', 'help']\n",
            "2-gram top: ['foster care', 'sounds like', 'even though', 'put username', 'sorry loss', 'feel free', 'feels like', 'easier said', 'feel like', 'former foster', 'foster youth', 'go back', 'people know', 'said done', 'sorry going', 'wish best', '20 years', 'beautiful day', 'buy something', 'care even']\n",
            "3-gram top: ['easier said done', 'ever need someone', 'feel free dm', 'one year care', '10 years like', '10x definitely blessing', '12 best answers', '13 years hurts', '14 imagining future', '16 especially home', '16 matter wonderful', '18 held resentment', '18 moved another', '20 years ago', '20 years every', '20s man felt', '25 still close', '30 hard resentful', '34 still get', '70 horses boys']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mN1TkSA3KOd2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}