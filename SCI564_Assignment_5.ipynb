{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SCI564_Assignment_5.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tawfiqam/MI564/blob/main/SCI564_Assignment_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WomcSdHU9lU2"
      },
      "source": [
        "#Part I: Data Collection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Qz-ch6U9rKY"
      },
      "source": [
        "# **Please forget about the Pushshift API. I will keep the code in case the API comes back to life (doubted). **\n",
        "\n",
        "**The dataset used in this analysis is [here](https://drive.google.com/file/d/1JAYNVL1mVI-a2EbDKvRl0U0-xuivQ_-R/view?usp=sharing). Download it and then use it in the analysis. Basically read it as a json file to a pandas dataframe. **\n",
        "\n",
        "[5 points]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmKd2BeoBTRf"
      },
      "source": [
        "#A1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5U5GUwaCgDQ"
      },
      "source": [
        "#Part II: Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3PAZKjrCjWY"
      },
      "source": [
        "Q2. In the first pass, clean the text in body by removing the newline character code, the aperasand character code, non-breaking space and, and zero-width space. Call this new column \"clean_text\" [5 points]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weaUt60TDUMT"
      },
      "source": [
        "#A2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95k_fYayDBUy"
      },
      "source": [
        "Q3.In the second pass, tokenize, remove stopwords and punctuation as well as urls (web links). I also want you to use the nltk lemmatizer to lemmatize the body [5 points]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CG3jaHYSDkBk"
      },
      "source": [
        "#A3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hklf8jUuEeKG"
      },
      "source": [
        "#Now that we have the cleaned text for each thread\n",
        "#documents, or docs represents each thread\n",
        "docs = df.clean_text.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sugc-Tsiyk-Z"
      },
      "source": [
        "##PartIII:Creating the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B02z_6UOEoDB"
      },
      "source": [
        "Q4. Using the documents, train a word2vec model using CBOW, with size=300. Each word should be repeated at least 5 times in the corpus to include it in the model. The window for the model should be 5. Use 6 cores (workers) for the analysis. Call your word2vec model model.\n",
        "\n",
        "[20 points]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqSG7iQgEr0e"
      },
      "source": [
        "#A4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF33B0PAE1ni"
      },
      "source": [
        "Q5.What are the closest 5 words to the word \"foster\"?\n",
        "\n",
        "\n",
        " [5 points]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHpOV_HvEyWF"
      },
      "source": [
        "#A5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-5pShwNyhPT"
      },
      "source": [
        "Q6. Using t-SNE to reduce the dimensionality, graph the closest words to \"foster\" [5 points]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QT2Em7tl5qdv"
      },
      "source": [
        "#A6"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApldPRwa5wuK"
      },
      "source": [
        "Q7. How similar (close) are the words \"foster\" and \"parent\"? [5 points]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpfsMhtH56mN"
      },
      "source": [
        "#A7"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaNaSDIo6BYF"
      },
      "source": [
        "Q8. Which of the terms \"parent\", \"shop\", and \"school\" do not fit? Use the doesnt_match feature here [5 points]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-Xxj7i26RIW"
      },
      "source": [
        "#A8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDFUNtXy6gTT"
      },
      "source": [
        "Q9. Create a new dataframe, call it result. Result should include every word in the word2vec model you created and the 300 features that represent it in multi-dimensional space [20 points]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5pdawdq6x-Q"
      },
      "source": [
        "#A9"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xl5-LLfg7Fwn"
      },
      "source": [
        "Q10.Using result.values, get the matrix that represents all the words in the model. Call this matrix X.\n",
        "\n",
        "Now, using t-SNE, reduce the dimensionality from 300 to 2.\n",
        "\n",
        "[10 points]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHkZiFUx7kvh"
      },
      "source": [
        "#A10."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9HfozbQ7mZC"
      },
      "source": [
        "##Part III: Visualizing a word2vec model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSTLJ6qC7xwg"
      },
      "source": [
        "Q11. Create a new word2vec model just as you did in Q4. In this case though, only include words that appear more than 15 times in our vocabulary. Call this model \"small model\" [5 points]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2VlRhUT7vya"
      },
      "source": [
        "#A11."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEE8kpkd8Sz2"
      },
      "source": [
        "Q12. Using t-SNE, reduce dimensionality to 2 dimensions and visualize them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Og13hRBG8Rq1"
      },
      "source": [
        "#A12"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}